from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import OpenAI
import os
import logging
import json
import httpx
from typing import Dict, Any, Optional, List

app = FastAPI()

# C·∫•u h√¨nh CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# C·∫•u h√¨nh Node.js API URL ƒë·ªÉ l·∫•y schema
NODE_API_URL = os.getenv("NODE_API_URL", "http://localhost:3000")

# üîπ D√πng OpenRouter endpoint
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY")
)

class ChatMessage(BaseModel):
    role: str  # "user" or "assistant"
    content: str

class ChatRequest(BaseModel):
    question: str
    conversation_history: Optional[List[ChatMessage]] = None  # L·ªãch s·ª≠ h·ªôi tho·∫°i

class FormatAnswerRequest(BaseModel):
    question: str
    query_result: list
    query_info: Optional[Dict[str, Any]] = None
    conversation_history: Optional[List[ChatMessage]] = None

class ChatAIService:
    """
    Service ch√≠nh x·ª≠ l√Ω chat AI v·ªõi kh·∫£ nƒÉng query database
    """
    
    def __init__(self):
        self.schema_cache = None
        self.schema_cache_time = None
    
    def _get_database_schema(self) -> str:
        """
        L·∫•y database schema t·ª´ Node.js API
        """
        try:
            # Cache schema trong 1 gi·ªù
            import time
            if self.schema_cache and self.schema_cache_time:
                if time.time() - self.schema_cache_time < 3600:
                    logger.info("Using cached schema")
                    return self.schema_cache
            
            logger.info(f"Fetching schema from: {NODE_API_URL}/api/v1/chat-ai/schema")
            with httpx.Client(timeout=10.0) as client:
                response = client.get(f"{NODE_API_URL}/api/v1/chat-ai/schema?format=text")
                logger.info(f"Schema API response status: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    schema = data.get("data", {}).get("schema", "")
                    if schema:
                        self.schema_cache = schema
                        self.schema_cache_time = time.time()
                        logger.info(f"Schema fetched successfully, length: {len(schema)} characters")
                        return schema
                    else:
                        logger.warning("Schema response is empty")
                        return ""
                else:
                    logger.warning(f"Failed to get schema: {response.status_code}, response: {response.text[:200]}")
                    return ""
        except Exception as e:
            logger.error(f"Error getting schema: {e}", exc_info=True)
            return ""
    
    def _decide_if_needs_database(self, question: str) -> bool:
        """
        Decision layer: Quy·∫øt ƒë·ªãnh xem c√¢u h·ªèi c√≥ c·∫ßn query database kh√¥ng
        S·ª≠ d·ª•ng keyword matching tr∆∞·ªõc (nhanh h∆°n), sau ƒë√≥ m·ªõi d√πng AI n·∫øu c·∫ßn
        """
        question_lower = question.lower()
        
        # Keyword matching - nhanh v√† ch√≠nh x√°c cho c√°c tr∆∞·ªùng h·ª£p ph·ªï bi·∫øn
        strong_db_keywords = [
            "c√≥ bao nhi√™u", "th·ªëng k√™", "danh s√°ch", "li·ªát k√™",
            "hi·ªÉn th·ªã", "show", "list", "ƒë·∫øm", "count"
        ]
        
        # N·∫øu c√≥ strong keywords, ch·∫Øc ch·∫Øn c·∫ßn DB
        if any(keyword in question_lower for keyword in strong_db_keywords):
            logger.info(f"Strong DB keyword detected in: '{question}'")
            return True
        
        # Ki·ªÉm tra c√≥ t·ª´ kh√≥a v·ªÅ entities (kh√≥a h·ªçc, b√†i t·∫≠p, etc.)
        entity_keywords = [
            "kh√≥a h·ªçc", "course", "b√†i t·∫≠p", "problem", "t√†i li·ªáu", "document",
            "ng∆∞·ªùi d√πng", "user", "cu·ªôc thi", "contest", "h·ªá th·ªëng"
        ]
        
        has_entity = any(keyword in question_lower for keyword in entity_keywords)
        
        # N·∫øu c√≥ entity keywords + c√°c t·ª´ ch·ªâ th·ªã, c·∫ßn DB
        if has_entity:
            indicator_keywords = [
                "c√≥", "trong", "c·ªßa", "n√†o", "g√¨", "hi·ªán t·∫°i", "hi·ªán c√≥",
                "m·ªõi nh·∫•t", "c≈© nh·∫•t", "nhi·ªÅu nh·∫•t", "√≠t nh·∫•t", "top"
            ]
            if any(keyword in question_lower for keyword in indicator_keywords):
                logger.info(f"Entity + indicator detected in: '{question}'")
                return True
        
        # N·∫øu kh√¥ng match keyword, d√πng AI ƒë·ªÉ quy·∫øt ƒë·ªãnh (cho c√°c tr∆∞·ªùng h·ª£p ph·ª©c t·∫°p)
        try:
            decision_prompt = (
                f"Ph√¢n t√≠ch c√¢u h·ªèi sau v√† quy·∫øt ƒë·ªãnh xem c√≥ c·∫ßn query database kh√¥ng:\n\n"
                f"C√¢u h·ªèi: {question}\n\n"
                f"C√°c lo·∫°i c√¢u h·ªèi C·∫¶N query database:\n"
                f"- H·ªèi v·ªÅ s·ªë l∆∞·ª£ng, th·ªëng k√™ (v√≠ d·ª•: 'c√≥ bao nhi√™u kh√≥a h·ªçc', 'th·ªëng k√™ ng∆∞·ªùi d√πng')\n"
                f"- H·ªèi v·ªÅ danh s√°ch, li·ªát k√™ (v√≠ d·ª•: 'danh s√°ch kh√≥a h·ªçc', 'hi·ªÉn th·ªã b√†i t·∫≠p')\n"
                f"- H·ªèi v·ªÅ th√¥ng tin c·ª• th·ªÉ t·ª´ h·ªá th·ªëng (v√≠ d·ª•: 'kh√≥a h·ªçc n√†o c√≥ rating cao nh·∫•t', 'b√†i t·∫≠p kh√≥ nh·∫•t')\n"
                f"- H·ªèi v·ªÅ d·ªØ li·ªáu th·ª±c t·∫ø trong h·ªá th·ªëng\n\n"
                f"C√°c lo·∫°i c√¢u h·ªèi KH√îNG C·∫¶N query database:\n"
                f"- H·ªèi v·ªÅ kh√°i ni·ªám, ƒë·ªãnh nghƒ©a (v√≠ d·ª•: 'Python l√† g√¨', 'thu·∫≠t to√°n quicksort l√† g√¨')\n"
                f"- H·ªèi v·ªÅ c√°ch l√†m, h∆∞·ªõng d·∫´n (v√≠ d·ª•: 'l√†m th·∫ø n√†o ƒë·ªÉ h·ªçc l·∫≠p tr√¨nh', 'c√°ch debug code')\n"
                f"- H·ªèi v·ªÅ l√Ω thuy·∫øt, ki·∫øn th·ª©c chung\n\n"
                f"Tr·∫£ l·ªùi CH·ªà b·∫±ng 'YES' n·∫øu c·∫ßn query database, ho·∫∑c 'NO' n·∫øu kh√¥ng c·∫ßn."
            )
            
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "B·∫°n l√† m·ªôt h·ªá th·ªëng ph√¢n t√≠ch c√¢u h·ªèi. Nhi·ªám v·ª• c·ªßa b·∫°n l√† quy·∫øt ƒë·ªãnh xem c√¢u h·ªèi c√≥ c·∫ßn query database kh√¥ng. Tr·∫£ l·ªùi CH·ªà b·∫±ng 'YES' ho·∫∑c 'NO'."
                    },
                    {"role": "user", "content": decision_prompt}
                ],
                temperature=0.1,
                max_tokens=10
            )
            
            decision = completion.choices[0].message.content.strip().upper()
            needs_db = decision == "YES" or "YES" in decision
            
            logger.info(f"AI decision for question '{question}': {'NEEDS_DB' if needs_db else 'NO_DB'} (response: {decision})")
            return needs_db
            
        except Exception as e:
            logger.error(f"Error in AI decision layer: {e}")
            # Fallback: n·∫øu c√≥ entity keywords th√¨ c·∫ßn DB
            return has_entity
    
    def process_question(self, question: str, conversation_history: Optional[List[Dict[str, str]]] = None) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω c√¢u h·ªèi v·ªõi decision layer: quy·∫øt ƒë·ªãnh c√≥ c·∫ßn query DB kh√¥ng
        conversation_history: List of messages v·ªõi format [{"role": "user|assistant", "content": "..."}]
        """
        try:
            logger.info(f"Processing question: {question}")
            if conversation_history:
                logger.info(f"Conversation history: {len(conversation_history)} messages")
            
            # Decision layer: Ki·ªÉm tra xem c√≥ c·∫ßn query database kh√¥ng
            needs_database = self._decide_if_needs_database(question)
            logger.info(f"Decision result: needs_database={needs_database} for question: '{question}'")
            
            if needs_database:
                # L·∫•y schema v√† sinh SQL
                logger.info("Fetching database schema...")
                schema = self._get_database_schema()
                
                if not schema:
                    logger.warning("Could not fetch schema, trying to generate SQL without schema...")
                    # Th·ª≠ sinh SQL v·ªõi basic schema info
                    basic_schema = (
                        "Tables: courses (id, title, description, rating, students, status, is_deleted), "
                        "problems (id, title, difficulty, is_deleted), "
                        "documents (id, title, description, is_deleted), "
                        "users (id, name, email, role, is_active)"
                    )
                    schema = basic_schema
                
                logger.info(f"Using schema, length: {len(schema)} characters")
                sql_result = self._generate_sql(question, schema, conversation_history)
                
                if sql_result.get("sql"):
                    logger.info(f"SQL generated successfully: {sql_result['sql']}")
                    return {
                        "answer": sql_result.get("fallback_answer", ""),
                        "data_source": "ai",
                        "requires_sql": True,
                        "sql": sql_result["sql"],
                        "query_info": sql_result.get("query_info", {})
                    }
                else:
                    logger.warning("Could not generate SQL, using fallback answer")
                    # Tr·∫£ v·ªÅ fallback answer n·∫øu c√≥
                    if sql_result.get("fallback_answer"):
                        return {
                            "answer": sql_result["fallback_answer"],
                            "data_source": "ai",
                            "requires_sql": False
                        }
            
            # Kh√¥ng c·∫ßn query DB ho·∫∑c kh√¥ng sinh ƒë∆∞·ª£c SQL, tr·∫£ l·ªùi b·∫±ng AI th√¥ng th∆∞·ªùng
            logger.info("Using standard AI response")
            ai_response = self._call_ai_with_history(question, conversation_history)
            
            return {
                "answer": ai_response,
                "data_source": "ai",
                "requires_sql": False
            }
            
        except Exception as e:
            logger.error(f"Error processing question: {e}", exc_info=True)
            return {
                "answer": "Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i.",
                "error": str(e),
                "requires_sql": False
            }
    
    def _generate_sql(self, question: str, schema: str, conversation_history: Optional[List[Dict[str, str]]] = None) -> Dict[str, Any]:
        """
        Sinh SQL query t·ª´ c√¢u h·ªèi ng∆∞·ªùi d√πng v·ªõi schema context
        """
        try:
            # R√∫t ng·∫Øn schema n·∫øu qu√° d√†i (gi·ªõi h·∫°n 8000 tokens)
            if len(schema) > 8000:
                schema = schema[:8000] + "\n... (schema truncated)"
            
            system_prompt = (
                "B·∫°n l√† m·ªôt chuy√™n gia SQL cho MySQL database. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch c√¢u h·ªèi ti·∫øng Vi·ªát "
                "v√† sinh ra c√¢u l·ªánh SQL SELECT ph√π h·ª£p.\n\n"
                "QUAN TR·ªåNG:\n"
                "- CH·ªà sinh ra c√¢u l·ªánh SELECT, KH√îNG ƒë∆∞·ª£c c√≥ c√°c l·ªánh kh√°c (INSERT, UPDATE, DELETE, DROP, etc.)\n"
                "- Ph·∫£i s·ª≠ d·ª•ng ƒë√∫ng t√™n b·∫£ng v√† c·ªôt t·ª´ schema ƒë∆∞·ª£c cung c·∫•p\n"
                "- T√™n b·∫£ng trong database l√†: courses (kh√≥a h·ªçc), problems (b√†i t·∫≠p), documents (t√†i li·ªáu), users (ng∆∞·ªùi d√πng)\n"
                "- Lu√¥n th√™m LIMIT ƒë·ªÉ gi·ªõi h·∫°n k·∫øt qu·∫£ (t·ªëi ƒëa 100 rows)\n"
                "- ƒê·ªëi v·ªõi c√¢u h·ªèi ƒë·∫øm s·ªë l∆∞·ª£ng, s·ª≠ d·ª•ng COUNT(*)\n"
                "- ƒê·ªëi v·ªõi c√¢u h·ªèi 'c√≥ bao nhi√™u', tr·∫£ v·ªÅ SELECT COUNT(*) as total FROM ...\n"
                "- Tr·∫£ v·ªÅ CH·ªà SQL query, kh√¥ng c√≥ gi·∫£i th√≠ch hay text kh√°c\n"
                "- N·∫øu kh√¥ng th·ªÉ sinh SQL h·ª£p l·ªá, tr·∫£ v·ªÅ 'NO_SQL'\n\n"
                f"DATABASE SCHEMA:\n{schema}\n\n"
                "V√≠ d·ª•:\n"
                "C√¢u h·ªèi: 'C√≥ nh·ªØng kh√≥a h·ªçc n√†o?'\n"
                "SQL: SELECT id, title, description, rating, students FROM courses WHERE is_deleted = false AND status = 'published' LIMIT 100\n\n"
                "C√¢u h·ªèi: 'H·ªá th·ªëng hi·ªán t·∫°i c√≥ bao nhi√™u kh√≥a h·ªçc?'\n"
                "SQL: SELECT COUNT(*) as total FROM courses WHERE is_deleted = false LIMIT 100\n\n"
                "C√¢u h·ªèi: 'C√≥ bao nhi√™u b√†i t·∫≠p kh√≥?'\n"
                "SQL: SELECT COUNT(*) as total FROM problems WHERE difficulty = 'Hard' AND is_deleted = false LIMIT 100\n\n"
            )
            
            user_prompt = f"C√¢u h·ªèi: {question}\n\nSQL:"
            
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,  # Lower temperature cho SQL generation
                max_tokens=500
            )
            
            sql_response = completion.choices[0].message.content.strip()
            logger.info(f"Raw SQL response from GPT: {sql_response[:200]}")
            
            # L√†m s·∫°ch SQL response
            sql_response = sql_response.replace("```sql", "").replace("```", "").strip()
            # Lo·∫°i b·ªè c√°c d√≤ng comment ho·∫∑c gi·∫£i th√≠ch
            lines = sql_response.split('\n')
            sql_lines = [line for line in lines if not line.strip().startswith('--') and line.strip()]
            sql_response = ' '.join(sql_lines).strip()
            
            logger.info(f"Cleaned SQL: {sql_response}")
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i SQL h·ª£p l·ªá kh√¥ng
            if sql_response.upper().startswith("SELECT") and "NO_SQL" not in sql_response.upper():
                logger.info(f"Valid SQL generated: {sql_response}")
                return {
                    "sql": sql_response,
                    "query_info": {
                        "type": "select",
                        "generated": True
                    }
                }
            else:
                logger.warning(f"Invalid SQL response: {sql_response}")
                # Fallback: tr·∫£ l·ªùi b·∫±ng AI th√¥ng th∆∞·ªùng v·ªõi conversation history
                if conversation_history:
                    fallback_answer = self._call_ai_with_history(question, conversation_history)
                else:
                    fallback_prompt = (
                        f"Ng∆∞·ªùi d√πng h·ªèi: {question}\n\n"
                        f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát. "
                        f"B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh."
                    )
                    fallback_answer = self._call_ai(fallback_prompt)
                
                return {
                    "sql": None,
                    "fallback_answer": fallback_answer,
                    "query_info": {
                        "type": "fallback",
                        "reason": "Could not generate valid SQL"
                    }
                }
                
        except Exception as e:
            logger.error(f"Error generating SQL: {e}")
            # Fallback v·ªõi conversation history
            if conversation_history:
                fallback_answer = self._call_ai_with_history(question, conversation_history)
            else:
                fallback_prompt = (
                    f"Ng∆∞·ªùi d√πng h·ªèi: {question}\n\n"
                    f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát. "
                    f"B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh."
                )
                fallback_answer = self._call_ai(fallback_prompt)
            
            return {
                "sql": None,
                "fallback_answer": fallback_answer,
                "query_info": {
                    "type": "fallback",
                    "error": str(e)
                }
            }
    
    def _call_ai(self, prompt: str) -> str:
        """
        G·ªçi AI ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi (non-streaming) - backward compatible
        """
        return self._call_ai_with_history(prompt, None)
    
    def _call_ai_with_history(self, question: str, conversation_history: Optional[List[Dict[str, str]]] = None) -> str:
        """
        G·ªçi AI v·ªõi conversation history
        """
        try:
            # Build messages array
            messages = [
                {
                    "role": "system", 
                    "content": (
                        "B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh, n√≥i ti·∫øng Vi·ªát. "
                        "B·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ l·∫≠p tr√¨nh, thu·∫≠t to√°n, c√¥ng ngh·ªá, "
                        "v√† c√°c ch·ªß ƒë·ªÅ li√™n quan ƒë·∫øn h·ªçc l·∫≠p tr√¨nh. "
                        "H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch. "
                        "N·∫øu kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, h√£y ƒë∆∞a ra g·ª£i √Ω ho·∫∑c h∆∞·ªõng d·∫´n t√¨m hi·ªÉu th√™m. "
                        "B·∫°n c√≥ th·ªÉ nh·ªõ v√† tham kh·∫£o c√°c c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi tr∆∞·ªõc ƒë√≥ trong cu·ªôc h·ªôi tho·∫°i."
                    )
                }
            ]
            
            # Th√™m conversation history n·∫øu c√≥ (gi·ªõi h·∫°n 20 messages g·∫ßn nh·∫•t ƒë·ªÉ tr√°nh qu√° d√†i)
            if conversation_history and len(conversation_history) > 0:
                logger.info(f"[_call_ai_with_history] Adding {len(conversation_history)} messages to conversation history")
                # Ch·ªâ l·∫•y 20 messages g·∫ßn nh·∫•t
                recent_history = conversation_history[-20:] if len(conversation_history) > 20 else conversation_history
                added_count = 0
                for msg in recent_history:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role in ["user", "assistant"] and content and content.strip():
                        messages.append({
                            "role": role,
                            "content": content.strip()
                        })
                        added_count += 1
                        logger.info(f"[_call_ai_with_history] Added message #{added_count}: {role} - {content[:100]}...")
                logger.info(f"[_call_ai_with_history] Total messages added: {added_count}")
            else:
                logger.info("[_call_ai_with_history] No conversation history provided")
            
            # Th√™m c√¢u h·ªèi hi·ªán t·∫°i
            messages.append({
                "role": "user",
                "content": question
            })
            
            logger.info(f"[_call_ai_with_history] Total messages sent to GPT: {len(messages)}")
            logger.info(f"[_call_ai_with_history] Messages structure: {[{'role': m['role'], 'content_length': len(m['content'])} for m in messages]}")
            
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            )
            
            logger.info(f"[_call_ai_with_history] GPT response received successfully")
            
            return completion.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Error calling AI: {e}")
            return "Xin l·ªói, t√¥i g·∫∑p l·ªói khi t·∫°o ph·∫£n h·ªìi. Vui l√≤ng th·ª≠ l·∫°i."
    
    def _stream_ai(self, prompt: str):
        """
        G·ªçi AI v·ªõi streaming response (backward compatible - kh√¥ng c√≥ history)
        """
        return self._stream_ai_with_history(prompt, None)
    
    def _stream_ai_with_history(self, question: str, conversation_history: Optional[List[Dict[str, str]]] = None):
        """
        G·ªçi AI v·ªõi streaming response v√† conversation history
        """
        try:
            # Build messages array
            messages = [
                {
                    "role": "system", 
                    "content": (
                        "B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh, n√≥i ti·∫øng Vi·ªát. "
                        "B·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ l·∫≠p tr√¨nh, thu·∫≠t to√°n, c√¥ng ngh·ªá, "
                        "v√† c√°c ch·ªß ƒë·ªÅ li√™n quan ƒë·∫øn h·ªçc l·∫≠p tr√¨nh. "
                        "H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch. "
                        "N·∫øu kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, h√£y ƒë∆∞a ra g·ª£i √Ω ho·∫∑c h∆∞·ªõng d·∫´n t√¨m hi·ªÉu th√™m. "
                        "B·∫°n c√≥ th·ªÉ nh·ªõ v√† tham kh·∫£o c√°c c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi tr∆∞·ªõc ƒë√≥ trong cu·ªôc h·ªôi tho·∫°i."
                    )
                }
            ]
            
            # Th√™m conversation history n·∫øu c√≥ (gi·ªõi h·∫°n 20 messages g·∫ßn nh·∫•t)
            if conversation_history and len(conversation_history) > 0:
                logger.info(f"[_stream_ai_with_history] Adding {len(conversation_history)} messages to conversation history")
                recent_history = conversation_history[-20:] if len(conversation_history) > 20 else conversation_history
                added_count = 0
                for msg in recent_history:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role in ["user", "assistant"] and content and content.strip():
                        messages.append({
                            "role": role,
                            "content": content.strip()
                        })
                        added_count += 1
                        logger.info(f"[_stream_ai_with_history] Added message #{added_count}: {role} - {content[:100]}...")
                logger.info(f"[_stream_ai_with_history] Total messages added: {added_count}")
            else:
                logger.info("[_stream_ai_with_history] No conversation history provided")
            
            # Th√™m c√¢u h·ªèi hi·ªán t·∫°i
            messages.append({
                "role": "user",
                "content": question
            })
            
            logger.info(f"[_stream_ai_with_history] Total messages sent to GPT: {len(messages)}")
            logger.info(f"[_stream_ai_with_history] Messages structure: {[{'role': m['role'], 'content_length': len(m['content'])} for m in messages]}")
            
            stream = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
                stream=True
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    yield chunk.choices[0].delta.content
            
            logger.info(f"[_stream_ai_with_history] Streaming completed successfully")
                    
        except Exception as e:
            logger.error(f"Error streaming AI with history: {e}", exc_info=True)
            yield "Xin l·ªói, t√¥i g·∫∑p l·ªói khi t·∫°o ph·∫£n h·ªìi. Vui l√≤ng th·ª≠ l·∫°i."
    
    def format_answer_from_query(self, question: str, query_result: list, query_info: Optional[Dict[str, Any]] = None, conversation_history: Optional[List[Dict[str, str]]] = None) -> str:
        """
        Format c√¢u tr·∫£ l·ªùi t·ª´ k·∫øt qu·∫£ query database
        """
        try:
            if not query_result or len(query_result) == 0:
                return "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n."
            
            # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho COUNT(*) queries
            first_result = query_result[0]
            if isinstance(first_result, dict) and 'total' in first_result:
                # ƒê√¢y l√† k·∫øt qu·∫£ COUNT(*)
                total = first_result.get('total', 0)
                logger.info(f"Formatting COUNT result: total={total}")
                
                # Format tr·ª±c ti·∫øp cho COUNT queries
                if 'kh√≥a h·ªçc' in question.lower() or 'course' in question.lower():
                    return f"Hi·ªán t·∫°i h·ªá th·ªëng c√≥ **{total}** kh√≥a h·ªçc."
                elif 'b√†i t·∫≠p' in question.lower() or 'problem' in question.lower():
                    return f"Hi·ªán t·∫°i h·ªá th·ªëng c√≥ **{total}** b√†i t·∫≠p."
                elif 't√†i li·ªáu' in question.lower() or 'document' in question.lower():
                    return f"Hi·ªán t·∫°i h·ªá th·ªëng c√≥ **{total}** t√†i li·ªáu."
                elif 'ng∆∞·ªùi d√πng' in question.lower() or 'user' in question.lower():
                    return f"Hi·ªán t·∫°i h·ªá th·ªëng c√≥ **{total}** ng∆∞·ªùi d√πng."
                else:
                    return f"D·ª±a tr√™n d·ªØ li·ªáu t·ª´ h·ªá th·ªëng, c√≥ **{total}** k·∫øt qu·∫£."
            
            # X·ª≠ l√Ω cho c√°c queries kh√°c (danh s√°ch, etc.)
            result_summary = json.dumps(query_result[:20], ensure_ascii=False, indent=2)  # Ch·ªâ l·∫•y 20 rows ƒë·∫ßu
            
            # Build messages v·ªõi conversation history
            messages = [
                {
                    "role": "system",
                    "content": (
                        "B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh, n√≥i ti·∫øng Vi·ªát. "
                        "B·∫°n c√≥ th·ªÉ nh·ªõ v√† tham kh·∫£o c√°c c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi tr∆∞·ªõc ƒë√≥ trong cu·ªôc h·ªôi tho·∫°i."
                    )
                }
            ]
            
            # Th√™m conversation history n·∫øu c√≥
            if conversation_history:
                recent_history = conversation_history[-5:] if len(conversation_history) > 5 else conversation_history
                for msg in recent_history:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role in ["user", "assistant"] and content:
                        messages.append({
                            "role": role,
                            "content": content
                        })
            
            # Th√™m c√¢u h·ªèi v√† k·∫øt qu·∫£ query
            messages.append({
                "role": "user",
                "content": (
                    f"Ng∆∞·ªùi d√πng ƒë√£ h·ªèi: {question}\n\n"
                    f"K·∫øt qu·∫£ t·ª´ database:\n{result_summary}\n\n"
                    f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n d·ªØ li·ªáu tr√™n m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát. "
                    f"H√£y tr√¨nh b√†y th√¥ng tin m·ªôt c√°ch d·ªÖ hi·ªÉu v√† c√≥ c·∫•u tr√∫c."
                )
            })
            
            try:
                completion = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    temperature=0.7,
                    max_tokens=1000
                )
                formatted_answer = completion.choices[0].message.content
            except Exception as e:
                logger.error(f"Error calling AI for formatting: {e}")
                # Fallback
                formatted_answer = f"D·ª±a tr√™n d·ªØ li·ªáu t·ª´ h·ªá th·ªëng:\n\n{result_summary}"
            return formatted_answer
            
        except Exception as e:
            logger.error(f"Error formatting answer: {e}", exc_info=True)
            # Fallback: format ƒë∆°n gi·∫£n
            if query_result and len(query_result) > 0:
                first_result = query_result[0]
                if isinstance(first_result, dict) and 'total' in first_result:
                    total = first_result.get('total', 0)
                    return f"Hi·ªán t·∫°i h·ªá th·ªëng c√≥ {total} k·∫øt qu·∫£."
                else:
                    return f"D·ª±a tr√™n d·ªØ li·ªáu t·ª´ h·ªá th·ªëng, t√¨m th·∫•y {len(query_result)} k·∫øt qu·∫£."
            else:
                return "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n."

# Kh·ªüi t·∫°o service
chat_ai_service = ChatAIService()

@app.post("/ask")
async def ask(request: ChatRequest):
    """
    API endpoint ch√≠nh ƒë·ªÉ x·ª≠ l√Ω c√¢u h·ªèi chat (non-streaming - gi·ªØ l·∫°i ƒë·ªÉ t∆∞∆°ng th√≠ch)
    """
    try:
        question = request.question.strip()
        
        if not question:
            return {
                "answer": "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω AI h·ªó tr·ª£ h·ªçc l·∫≠p tr√¨nh. B·∫°n mu·ªën h·ªèi g√¨?",
                "data_source": "ai"
            }
        
        # L·∫•y conversation history t·ª´ request
        conversation_history = None
        if hasattr(request, 'conversation_history') and request.conversation_history:
            conversation_history = [
                {"role": msg.role, "content": msg.content}
                for msg in request.conversation_history
            ]
            logger.info(f"[/ask] Received conversation_history: {len(conversation_history)} messages")
            if len(conversation_history) > 0:
                logger.info(f"[/ask] First message: {conversation_history[0]}")
                logger.info(f"[/ask] Last message: {conversation_history[-1]}")
        else:
            logger.info(f"[/ask] No conversation_history in request")
        
        # X·ª≠ l√Ω c√¢u h·ªèi th√¥ng qua ChatAI service
        result = chat_ai_service.process_question(question, conversation_history)
        
        return result
        
    except Exception as e:
        logger.error(f"Error in ask endpoint: {e}")
        return {
            "answer": "Xin l·ªói, c√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i sau.",
            "error": str(e)
        }

@app.post("/ask-stream")
async def ask_stream(request: ChatRequest):
    """
    API endpoint streaming ƒë·ªÉ x·ª≠ l√Ω c√¢u h·ªèi chat v·ªõi streaming response
    """
    try:
        question = request.question.strip()
        
        if not question:
            def empty_response():
                yield json.dumps({"type": "error", "content": "C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng"}) + "\n"
            return StreamingResponse(empty_response(), media_type="text/event-stream")
        
        # L·∫•y conversation history t·ª´ request
        conversation_history = None
        if hasattr(request, 'conversation_history') and request.conversation_history:
            conversation_history = [
                {"role": msg.role, "content": msg.content}
                for msg in request.conversation_history
            ]
            logger.info(f"[/ask-stream] Received conversation_history: {len(conversation_history)} messages")
            if len(conversation_history) > 0:
                logger.info(f"[/ask-stream] First message: {conversation_history[0]}")
                logger.info(f"[/ask-stream] Last message: {conversation_history[-1]}")
        else:
            logger.info(f"[/ask-stream] No conversation_history in request")
        
        def generate():
            try:
                for chunk in chat_ai_service._stream_ai_with_history(question, conversation_history):
                    # G·ª≠i t·ª´ng chunk d∆∞·ªõi d·∫°ng JSON
                    data = json.dumps({"type": "chunk", "content": chunk}, ensure_ascii=False)
                    yield f"data: {data}\n\n"
                
                # G·ª≠i signal k·∫øt th√∫c
                yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                logger.error(f"Error in stream generation: {e}")
                error_data = json.dumps({"type": "error", "content": "C√≥ l·ªói x·∫£y ra khi t·∫°o ph·∫£n h·ªìi"}, ensure_ascii=False)
                yield f"data: {error_data}\n\n"
        
        return StreamingResponse(generate(), media_type="text/event-stream")
        
    except Exception as e:
        logger.error(f"Error in ask_stream endpoint: {e}")
        def error_response():
            yield json.dumps({"type": "error", "content": "C√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i sau."}, ensure_ascii=False) + "\n"
        return StreamingResponse(error_response(), media_type="text/event-stream")

@app.post("/format-answer")
async def format_answer(request: FormatAnswerRequest):
    """
    Format c√¢u tr·∫£ l·ªùi t·ª´ k·∫øt qu·∫£ query database
    """
    try:
        # L·∫•y conversation history t·ª´ request
        conversation_history = None
        if request.conversation_history:
            conversation_history = [
                {"role": msg.role, "content": msg.content}
                for msg in request.conversation_history
            ]
            logger.info(f"[/format-answer] Received conversation_history: {len(conversation_history)} messages")
            if len(conversation_history) > 0:
                logger.info(f"[/format-answer] First message: {conversation_history[0]}")
                logger.info(f"[/format-answer] Last message: {conversation_history[-1]}")
        else:
            logger.info(f"[/format-answer] No conversation_history in request")
        
        formatted = chat_ai_service.format_answer_from_query(
            request.question,
            request.query_result,
            request.query_info,
            conversation_history
        )
        
        return {
            "answer": formatted,
            "data_source": "database"
        }
        
    except Exception as e:
        logger.error(f"Error in format_answer endpoint: {e}")
        return {
            "answer": "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi format c√¢u tr·∫£ l·ªùi.",
            "error": str(e)
        }

@app.get("/health")
async def health_check():
    """
    Health check endpoint
    """
    try:
        # Ki·ªÉm tra k·∫øt n·ªëi AI
        test_response = chat_ai_service._call_ai("Test")
        
        return {
            "status": "healthy",
            "ai_client": "active",
            "message": "AI service is running"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "message": "AI service is not available"
        }
