from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import OpenAI
import os
import logging
import json
import httpx
from typing import Dict, Any, Optional

app = FastAPI()

# C·∫•u h√¨nh CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# C·∫•u h√¨nh Node.js API URL ƒë·ªÉ l·∫•y schema
NODE_API_URL = os.getenv("NODE_API_URL", "http://localhost:3000")

# üîπ D√πng OpenRouter endpoint
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY")
)

class ChatRequest(BaseModel):
    question: str

class FormatAnswerRequest(BaseModel):
    question: str
    query_result: list
    query_info: Optional[Dict[str, Any]] = None

class ChatAIService:
    """
    Service ch√≠nh x·ª≠ l√Ω chat AI v·ªõi kh·∫£ nƒÉng query database
    """
    
    def __init__(self):
        self.schema_cache = None
        self.schema_cache_time = None
    
    def _get_database_schema(self) -> str:
        """
        L·∫•y database schema t·ª´ Node.js API
        """
        try:
            # Cache schema trong 1 gi·ªù
            import time
            if self.schema_cache and self.schema_cache_time:
                if time.time() - self.schema_cache_time < 3600:
                    logger.info("Using cached schema")
                    return self.schema_cache
            
            logger.info(f"Fetching schema from: {NODE_API_URL}/api/v1/chat-ai/schema")
            with httpx.Client(timeout=10.0) as client:
                response = client.get(f"{NODE_API_URL}/api/v1/chat-ai/schema?format=text")
                logger.info(f"Schema API response status: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    schema = data.get("data", {}).get("schema", "")
                    if schema:
                        self.schema_cache = schema
                        self.schema_cache_time = time.time()
                        logger.info(f"Schema fetched successfully, length: {len(schema)} characters")
                        return schema
                    else:
                        logger.warning("Schema response is empty")
                        return ""
                else:
                    logger.warning(f"Failed to get schema: {response.status_code}, response: {response.text[:200]}")
                    return ""
        except Exception as e:
            logger.error(f"Error getting schema: {e}", exc_info=True)
            return ""
    
    def _decide_if_needs_database(self, question: str) -> bool:
        """
        Decision layer: Quy·∫øt ƒë·ªãnh xem c√¢u h·ªèi c√≥ c·∫ßn query database kh√¥ng
        S·ª≠ d·ª•ng keyword matching tr∆∞·ªõc (nhanh h∆°n), sau ƒë√≥ m·ªõi d√πng AI n·∫øu c·∫ßn
        """
        question_lower = question.lower()
        
        # Keyword matching - nhanh v√† ch√≠nh x√°c cho c√°c tr∆∞·ªùng h·ª£p ph·ªï bi·∫øn
        strong_db_keywords = [
            "c√≥ bao nhi√™u", "th·ªëng k√™", "danh s√°ch", "li·ªát k√™",
            "hi·ªÉn th·ªã", "show", "list", "ƒë·∫øm", "count"
        ]
        
        # N·∫øu c√≥ strong keywords, ch·∫Øc ch·∫Øn c·∫ßn DB
        if any(keyword in question_lower for keyword in strong_db_keywords):
            logger.info(f"Strong DB keyword detected in: '{question}'")
            return True
        
        # Ki·ªÉm tra c√≥ t·ª´ kh√≥a v·ªÅ entities (kh√≥a h·ªçc, b√†i t·∫≠p, etc.)
        entity_keywords = [
            "kh√≥a h·ªçc", "course", "b√†i t·∫≠p", "problem", "t√†i li·ªáu", "document",
            "ng∆∞·ªùi d√πng", "user", "cu·ªôc thi", "contest", "h·ªá th·ªëng"
        ]
        
        has_entity = any(keyword in question_lower for keyword in entity_keywords)
        
        # N·∫øu c√≥ entity keywords + c√°c t·ª´ ch·ªâ th·ªã, c·∫ßn DB
        if has_entity:
            indicator_keywords = [
                "c√≥", "trong", "c·ªßa", "n√†o", "g√¨", "hi·ªán t·∫°i", "hi·ªán c√≥",
                "m·ªõi nh·∫•t", "c≈© nh·∫•t", "nhi·ªÅu nh·∫•t", "√≠t nh·∫•t", "top"
            ]
            if any(keyword in question_lower for keyword in indicator_keywords):
                logger.info(f"Entity + indicator detected in: '{question}'")
                return True
        
        # N·∫øu kh√¥ng match keyword, d√πng AI ƒë·ªÉ quy·∫øt ƒë·ªãnh (cho c√°c tr∆∞·ªùng h·ª£p ph·ª©c t·∫°p)
        try:
            decision_prompt = (
                f"Ph√¢n t√≠ch c√¢u h·ªèi sau v√† quy·∫øt ƒë·ªãnh xem c√≥ c·∫ßn query database kh√¥ng:\n\n"
                f"C√¢u h·ªèi: {question}\n\n"
                f"C√°c lo·∫°i c√¢u h·ªèi C·∫¶N query database:\n"
                f"- H·ªèi v·ªÅ s·ªë l∆∞·ª£ng, th·ªëng k√™ (v√≠ d·ª•: 'c√≥ bao nhi√™u kh√≥a h·ªçc', 'th·ªëng k√™ ng∆∞·ªùi d√πng')\n"
                f"- H·ªèi v·ªÅ danh s√°ch, li·ªát k√™ (v√≠ d·ª•: 'danh s√°ch kh√≥a h·ªçc', 'hi·ªÉn th·ªã b√†i t·∫≠p')\n"
                f"- H·ªèi v·ªÅ th√¥ng tin c·ª• th·ªÉ t·ª´ h·ªá th·ªëng (v√≠ d·ª•: 'kh√≥a h·ªçc n√†o c√≥ rating cao nh·∫•t', 'b√†i t·∫≠p kh√≥ nh·∫•t')\n"
                f"- H·ªèi v·ªÅ d·ªØ li·ªáu th·ª±c t·∫ø trong h·ªá th·ªëng\n\n"
                f"C√°c lo·∫°i c√¢u h·ªèi KH√îNG C·∫¶N query database:\n"
                f"- H·ªèi v·ªÅ kh√°i ni·ªám, ƒë·ªãnh nghƒ©a (v√≠ d·ª•: 'Python l√† g√¨', 'thu·∫≠t to√°n quicksort l√† g√¨')\n"
                f"- H·ªèi v·ªÅ c√°ch l√†m, h∆∞·ªõng d·∫´n (v√≠ d·ª•: 'l√†m th·∫ø n√†o ƒë·ªÉ h·ªçc l·∫≠p tr√¨nh', 'c√°ch debug code')\n"
                f"- H·ªèi v·ªÅ l√Ω thuy·∫øt, ki·∫øn th·ª©c chung\n\n"
                f"Tr·∫£ l·ªùi CH·ªà b·∫±ng 'YES' n·∫øu c·∫ßn query database, ho·∫∑c 'NO' n·∫øu kh√¥ng c·∫ßn."
            )
            
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "B·∫°n l√† m·ªôt h·ªá th·ªëng ph√¢n t√≠ch c√¢u h·ªèi. Nhi·ªám v·ª• c·ªßa b·∫°n l√† quy·∫øt ƒë·ªãnh xem c√¢u h·ªèi c√≥ c·∫ßn query database kh√¥ng. Tr·∫£ l·ªùi CH·ªà b·∫±ng 'YES' ho·∫∑c 'NO'."
                    },
                    {"role": "user", "content": decision_prompt}
                ],
                temperature=0.1,
                max_tokens=10
            )
            
            decision = completion.choices[0].message.content.strip().upper()
            needs_db = decision == "YES" or "YES" in decision
            
            logger.info(f"AI decision for question '{question}': {'NEEDS_DB' if needs_db else 'NO_DB'} (response: {decision})")
            return needs_db
            
        except Exception as e:
            logger.error(f"Error in AI decision layer: {e}")
            # Fallback: n·∫øu c√≥ entity keywords th√¨ c·∫ßn DB
            return has_entity
    
    def process_question(self, question: str) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω c√¢u h·ªèi v·ªõi decision layer: quy·∫øt ƒë·ªãnh c√≥ c·∫ßn query DB kh√¥ng
        """
        try:
            logger.info(f"Processing question: {question}")
            
            # Decision layer: Ki·ªÉm tra xem c√≥ c·∫ßn query database kh√¥ng
            needs_database = self._decide_if_needs_database(question)
            logger.info(f"Decision result: needs_database={needs_database} for question: '{question}'")
            
            if needs_database:
                # L·∫•y schema v√† sinh SQL
                logger.info("Fetching database schema...")
                schema = self._get_database_schema()
                
                if not schema:
                    logger.warning("Could not fetch schema, trying to generate SQL without schema...")
                    # Th·ª≠ sinh SQL v·ªõi basic schema info
                    basic_schema = (
                        "Tables: courses (id, title, description, rating, students, status, is_deleted), "
                        "problems (id, title, difficulty, is_deleted), "
                        "documents (id, title, description, is_deleted), "
                        "users (id, name, email, role, is_active)"
                    )
                    schema = basic_schema
                
                logger.info(f"Using schema, length: {len(schema)} characters")
                sql_result = self._generate_sql(question, schema)
                
                if sql_result.get("sql"):
                    logger.info(f"SQL generated successfully: {sql_result['sql']}")
                    return {
                        "answer": sql_result.get("fallback_answer", ""),
                        "data_source": "ai",
                        "requires_sql": True,
                        "sql": sql_result["sql"],
                        "query_info": sql_result.get("query_info", {})
                    }
                else:
                    logger.warning("Could not generate SQL, using fallback answer")
                    # Tr·∫£ v·ªÅ fallback answer n·∫øu c√≥
                    if sql_result.get("fallback_answer"):
                        return {
                            "answer": sql_result["fallback_answer"],
                            "data_source": "ai",
                            "requires_sql": False
                        }
            
            # Kh√¥ng c·∫ßn query DB ho·∫∑c kh√¥ng sinh ƒë∆∞·ª£c SQL, tr·∫£ l·ªùi b·∫±ng AI th√¥ng th∆∞·ªùng
            logger.info("Using standard AI response")
            prompt = (
                f"Ng∆∞·ªùi d√πng h·ªèi: {question}\n\n"
                f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát. "
                f"B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh."
            )
            
            ai_response = self._call_ai(prompt)
            
            return {
                "answer": ai_response,
                "data_source": "ai",
                "requires_sql": False
            }
            
        except Exception as e:
            logger.error(f"Error processing question: {e}", exc_info=True)
            return {
                "answer": "Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i.",
                "error": str(e),
                "requires_sql": False
            }
    
    def _generate_sql(self, question: str, schema: str) -> Dict[str, Any]:
        """
        Sinh SQL query t·ª´ c√¢u h·ªèi ng∆∞·ªùi d√πng v·ªõi schema context
        """
        try:
            # R√∫t ng·∫Øn schema n·∫øu qu√° d√†i (gi·ªõi h·∫°n 8000 tokens)
            if len(schema) > 8000:
                schema = schema[:8000] + "\n... (schema truncated)"
            
            system_prompt = (
                "B·∫°n l√† m·ªôt chuy√™n gia SQL cho MySQL database. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch c√¢u h·ªèi ti·∫øng Vi·ªát "
                "v√† sinh ra c√¢u l·ªánh SQL SELECT ph√π h·ª£p.\n\n"
                "QUAN TR·ªåNG:\n"
                "- CH·ªà sinh ra c√¢u l·ªánh SELECT, KH√îNG ƒë∆∞·ª£c c√≥ c√°c l·ªánh kh√°c (INSERT, UPDATE, DELETE, DROP, etc.)\n"
                "- Ph·∫£i s·ª≠ d·ª•ng ƒë√∫ng t√™n b·∫£ng v√† c·ªôt t·ª´ schema ƒë∆∞·ª£c cung c·∫•p\n"
                "- T√™n b·∫£ng trong database l√†: courses (kh√≥a h·ªçc), problems (b√†i t·∫≠p), documents (t√†i li·ªáu), users (ng∆∞·ªùi d√πng)\n"
                "- Lu√¥n th√™m LIMIT ƒë·ªÉ gi·ªõi h·∫°n k·∫øt qu·∫£ (t·ªëi ƒëa 100 rows)\n"
                "- ƒê·ªëi v·ªõi c√¢u h·ªèi ƒë·∫øm s·ªë l∆∞·ª£ng, s·ª≠ d·ª•ng COUNT(*)\n"
                "- ƒê·ªëi v·ªõi c√¢u h·ªèi 'c√≥ bao nhi√™u', tr·∫£ v·ªÅ SELECT COUNT(*) as total FROM ...\n"
                "- Tr·∫£ v·ªÅ CH·ªà SQL query, kh√¥ng c√≥ gi·∫£i th√≠ch hay text kh√°c\n"
                "- N·∫øu kh√¥ng th·ªÉ sinh SQL h·ª£p l·ªá, tr·∫£ v·ªÅ 'NO_SQL'\n\n"
                f"DATABASE SCHEMA:\n{schema}\n\n"
                "V√≠ d·ª•:\n"
                "C√¢u h·ªèi: 'C√≥ nh·ªØng kh√≥a h·ªçc n√†o?'\n"
                "SQL: SELECT id, title, description, rating, students FROM courses WHERE is_deleted = false AND status = 'published' LIMIT 100\n\n"
                "C√¢u h·ªèi: 'H·ªá th·ªëng hi·ªán t·∫°i c√≥ bao nhi√™u kh√≥a h·ªçc?'\n"
                "SQL: SELECT COUNT(*) as total FROM courses WHERE is_deleted = false LIMIT 100\n\n"
                "C√¢u h·ªèi: 'C√≥ bao nhi√™u b√†i t·∫≠p kh√≥?'\n"
                "SQL: SELECT COUNT(*) as total FROM problems WHERE difficulty = 'Hard' AND is_deleted = false LIMIT 100\n\n"
            )
            
            user_prompt = f"C√¢u h·ªèi: {question}\n\nSQL:"
            
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,  # Lower temperature cho SQL generation
                max_tokens=500
            )
            
            sql_response = completion.choices[0].message.content.strip()
            logger.info(f"Raw SQL response from GPT: {sql_response[:200]}")
            
            # L√†m s·∫°ch SQL response
            sql_response = sql_response.replace("```sql", "").replace("```", "").strip()
            # Lo·∫°i b·ªè c√°c d√≤ng comment ho·∫∑c gi·∫£i th√≠ch
            lines = sql_response.split('\n')
            sql_lines = [line for line in lines if not line.strip().startswith('--') and line.strip()]
            sql_response = ' '.join(sql_lines).strip()
            
            logger.info(f"Cleaned SQL: {sql_response}")
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i SQL h·ª£p l·ªá kh√¥ng
            if sql_response.upper().startswith("SELECT") and "NO_SQL" not in sql_response.upper():
                logger.info(f"Valid SQL generated: {sql_response}")
                return {
                    "sql": sql_response,
                    "query_info": {
                        "type": "select",
                        "generated": True
                    }
                }
            else:
                logger.warning(f"Invalid SQL response: {sql_response}")
                # Fallback: tr·∫£ l·ªùi b·∫±ng AI th√¥ng th∆∞·ªùng
                fallback_prompt = (
                    f"Ng∆∞·ªùi d√πng h·ªèi: {question}\n\n"
                    f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát. "
                    f"B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh."
                )
                fallback_answer = self._call_ai(fallback_prompt)
                
                return {
                    "sql": None,
                    "fallback_answer": fallback_answer,
                    "query_info": {
                        "type": "fallback",
                        "reason": "Could not generate valid SQL"
                    }
                }
                
        except Exception as e:
            logger.error(f"Error generating SQL: {e}")
            # Fallback
            fallback_prompt = (
                f"Ng∆∞·ªùi d√πng h·ªèi: {question}\n\n"
                f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát. "
                f"B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh."
            )
            fallback_answer = self._call_ai(fallback_prompt)
            
            return {
                "sql": None,
                "fallback_answer": fallback_answer,
                "query_info": {
                    "type": "fallback",
                    "error": str(e)
                }
            }
    
    def _call_ai(self, prompt: str) -> str:
        """
        G·ªçi AI ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi (non-streaming)
        """
        try:
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": (
                            "B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh, n√≥i ti·∫øng Vi·ªát. "
                            "B·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ l·∫≠p tr√¨nh, thu·∫≠t to√°n, c√¥ng ngh·ªá, "
                            "v√† c√°c ch·ªß ƒë·ªÅ li√™n quan ƒë·∫øn h·ªçc l·∫≠p tr√¨nh. "
                            "H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch. "
                            "N·∫øu kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, h√£y ƒë∆∞a ra g·ª£i √Ω ho·∫∑c h∆∞·ªõng d·∫´n t√¨m hi·ªÉu th√™m."
                        )
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            return completion.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Error calling AI: {e}")
            return "Xin l·ªói, t√¥i g·∫∑p l·ªói khi t·∫°o ph·∫£n h·ªìi. Vui l√≤ng th·ª≠ l·∫°i."
    
    def _stream_ai(self, prompt: str):
        """
        G·ªçi AI v·ªõi streaming response
        """
        try:
            stream = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": (
                            "B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh, n√≥i ti·∫øng Vi·ªát. "
                            "B·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ l·∫≠p tr√¨nh, thu·∫≠t to√°n, c√¥ng ngh·ªá, "
                            "v√† c√°c ch·ªß ƒë·ªÅ li√™n quan ƒë·∫øn h·ªçc l·∫≠p tr√¨nh. "
                            "H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch. "
                            "N·∫øu kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, h√£y ƒë∆∞a ra g·ª£i √Ω ho·∫∑c h∆∞·ªõng d·∫´n t√¨m hi·ªÉu th√™m."
                        )
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000,
                stream=True
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"Error streaming AI: {e}")
            yield "Xin l·ªói, t√¥i g·∫∑p l·ªói khi t·∫°o ph·∫£n h·ªìi. Vui l√≤ng th·ª≠ l·∫°i."
    
    def format_answer_from_query(self, question: str, query_result: list, query_info: Optional[Dict[str, Any]] = None) -> str:
        """
        Format c√¢u tr·∫£ l·ªùi t·ª´ k·∫øt qu·∫£ query database
        """
        try:
            if not query_result or len(query_result) == 0:
                return "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n."
            
            # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho COUNT(*) queries
            first_result = query_result[0]
            if isinstance(first_result, dict) and 'total' in first_result:
                # ƒê√¢y l√† k·∫øt qu·∫£ COUNT(*)
                total = first_result.get('total', 0)
                logger.info(f"Formatting COUNT result: total={total}")
                
                # Format tr·ª±c ti·∫øp cho COUNT queries
                if 'kh√≥a h·ªçc' in question.lower() or 'course' in question.lower():
                    return f"Hi·ªán t·∫°i h·ªá th·ªëng c√≥ **{total}** kh√≥a h·ªçc."
                elif 'b√†i t·∫≠p' in question.lower() or 'problem' in question.lower():
                    return f"Hi·ªán t·∫°i h·ªá th·ªëng c√≥ **{total}** b√†i t·∫≠p."
                elif 't√†i li·ªáu' in question.lower() or 'document' in question.lower():
                    return f"Hi·ªán t·∫°i h·ªá th·ªëng c√≥ **{total}** t√†i li·ªáu."
                elif 'ng∆∞·ªùi d√πng' in question.lower() or 'user' in question.lower():
                    return f"Hi·ªán t·∫°i h·ªá th·ªëng c√≥ **{total}** ng∆∞·ªùi d√πng."
                else:
                    return f"D·ª±a tr√™n d·ªØ li·ªáu t·ª´ h·ªá th·ªëng, c√≥ **{total}** k·∫øt qu·∫£."
            
            # X·ª≠ l√Ω cho c√°c queries kh√°c (danh s√°ch, etc.)
            result_summary = json.dumps(query_result[:20], ensure_ascii=False, indent=2)  # Ch·ªâ l·∫•y 20 rows ƒë·∫ßu
            
            prompt = (
                f"Ng∆∞·ªùi d√πng ƒë√£ h·ªèi: {question}\n\n"
                f"K·∫øt qu·∫£ t·ª´ database:\n{result_summary}\n\n"
                f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n d·ªØ li·ªáu tr√™n m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát. "
                f"B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh. "
                f"H√£y tr√¨nh b√†y th√¥ng tin m·ªôt c√°ch d·ªÖ hi·ªÉu v√† c√≥ c·∫•u tr√∫c."
            )
            
            formatted_answer = self._call_ai(prompt)
            return formatted_answer
            
        except Exception as e:
            logger.error(f"Error formatting answer: {e}", exc_info=True)
            # Fallback: format ƒë∆°n gi·∫£n
            if query_result and len(query_result) > 0:
                first_result = query_result[0]
                if isinstance(first_result, dict) and 'total' in first_result:
                    total = first_result.get('total', 0)
                    return f"Hi·ªán t·∫°i h·ªá th·ªëng c√≥ {total} k·∫øt qu·∫£."
                else:
                    return f"D·ª±a tr√™n d·ªØ li·ªáu t·ª´ h·ªá th·ªëng, t√¨m th·∫•y {len(query_result)} k·∫øt qu·∫£."
            else:
                return "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n."

# Kh·ªüi t·∫°o service
chat_ai_service = ChatAIService()

@app.post("/ask")
async def ask(request: ChatRequest):
    """
    API endpoint ch√≠nh ƒë·ªÉ x·ª≠ l√Ω c√¢u h·ªèi chat (non-streaming - gi·ªØ l·∫°i ƒë·ªÉ t∆∞∆°ng th√≠ch)
    """
    try:
        question = request.question.strip()
        
        if not question:
            return {
                "answer": "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω AI h·ªó tr·ª£ h·ªçc l·∫≠p tr√¨nh. B·∫°n mu·ªën h·ªèi g√¨?",
                "data_source": "ai"
            }
        
        # X·ª≠ l√Ω c√¢u h·ªèi th√¥ng qua ChatAI service
        result = chat_ai_service.process_question(question)
        
        return result
        
    except Exception as e:
        logger.error(f"Error in ask endpoint: {e}")
        return {
            "answer": "Xin l·ªói, c√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i sau.",
            "error": str(e)
        }

@app.post("/ask-stream")
async def ask_stream(request: ChatRequest):
    """
    API endpoint streaming ƒë·ªÉ x·ª≠ l√Ω c√¢u h·ªèi chat v·ªõi streaming response
    """
    try:
        question = request.question.strip()
        
        if not question:
            def empty_response():
                yield json.dumps({"type": "error", "content": "C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng"}) + "\n"
            return StreamingResponse(empty_response(), media_type="text/event-stream")
        
        def generate():
            try:
                for chunk in chat_ai_service._stream_ai(
                    f"Ng∆∞·ªùi d√πng h·ªèi: {question}\n\n"
                    f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát. "
                    f"B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh."
                ):
                    # G·ª≠i t·ª´ng chunk d∆∞·ªõi d·∫°ng JSON
                    data = json.dumps({"type": "chunk", "content": chunk}, ensure_ascii=False)
                    yield f"data: {data}\n\n"
                
                # G·ª≠i signal k·∫øt th√∫c
                yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                logger.error(f"Error in stream generation: {e}")
                error_data = json.dumps({"type": "error", "content": "C√≥ l·ªói x·∫£y ra khi t·∫°o ph·∫£n h·ªìi"}, ensure_ascii=False)
                yield f"data: {error_data}\n\n"
        
        return StreamingResponse(generate(), media_type="text/event-stream")
        
    except Exception as e:
        logger.error(f"Error in ask_stream endpoint: {e}")
        def error_response():
            yield json.dumps({"type": "error", "content": "C√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i sau."}, ensure_ascii=False) + "\n"
        return StreamingResponse(error_response(), media_type="text/event-stream")

@app.post("/format-answer")
async def format_answer(request: FormatAnswerRequest):
    """
    Format c√¢u tr·∫£ l·ªùi t·ª´ k·∫øt qu·∫£ query database
    """
    try:
        formatted = chat_ai_service.format_answer_from_query(
            request.question,
            request.query_result,
            request.query_info
        )
        
        return {
            "answer": formatted,
            "data_source": "database"
        }
        
    except Exception as e:
        logger.error(f"Error in format_answer endpoint: {e}")
        return {
            "answer": "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi format c√¢u tr·∫£ l·ªùi.",
            "error": str(e)
        }

@app.get("/health")
async def health_check():
    """
    Health check endpoint
    """
    try:
        # Ki·ªÉm tra k·∫øt n·ªëi AI
        test_response = chat_ai_service._call_ai("Test")
        
        return {
            "status": "healthy",
            "ai_client": "active",
            "message": "AI service is running"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "message": "AI service is not available"
        }
