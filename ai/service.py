from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from openai import OpenAI
import os
import logging
import json
from typing import Dict, Any

app = FastAPI()

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# üîπ D√πng OpenRouter endpoint
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY")
)

class ChatRequest(BaseModel):
    question: str

class ChatAIService:
    """
    Service ch√≠nh x·ª≠ l√Ω chat AI - ch·ªâ s·ª≠ d·ª•ng AI ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi
    """
    
    def __init__(self):
        pass
    
    def process_question(self, question: str) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω c√¢u h·ªèi v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ t·ª´ AI
        """
        try:
            # T·∫°o prompt cho AI
            prompt = (
                f"Ng∆∞·ªùi d√πng h·ªèi: {question}\n\n"
                f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát. "
                f"B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh."
            )
            
            # G·ªçi AI
            ai_response = self._call_ai(prompt)
            
            return {
                "answer": ai_response,
                "data_source": "ai"
            }
            
        except Exception as e:
            logger.error(f"Error processing question: {e}")
            return {
                "answer": "Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i.",
                "error": str(e)
            }
    
    def _call_ai(self, prompt: str) -> str:
        """
        G·ªçi AI ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi (non-streaming)
        """
        try:
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": (
                            "B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh, n√≥i ti·∫øng Vi·ªát. "
                            "B·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ l·∫≠p tr√¨nh, thu·∫≠t to√°n, c√¥ng ngh·ªá, "
                            "v√† c√°c ch·ªß ƒë·ªÅ li√™n quan ƒë·∫øn h·ªçc l·∫≠p tr√¨nh. "
                            "H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch. "
                            "N·∫øu kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, h√£y ƒë∆∞a ra g·ª£i √Ω ho·∫∑c h∆∞·ªõng d·∫´n t√¨m hi·ªÉu th√™m."
                        )
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            return completion.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Error calling AI: {e}")
            return "Xin l·ªói, t√¥i g·∫∑p l·ªói khi t·∫°o ph·∫£n h·ªìi. Vui l√≤ng th·ª≠ l·∫°i."
    
    def _stream_ai(self, prompt: str):
        """
        G·ªçi AI v·ªõi streaming response
        """
        try:
            stream = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": (
                            "B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh, n√≥i ti·∫øng Vi·ªát. "
                            "B·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ l·∫≠p tr√¨nh, thu·∫≠t to√°n, c√¥ng ngh·ªá, "
                            "v√† c√°c ch·ªß ƒë·ªÅ li√™n quan ƒë·∫øn h·ªçc l·∫≠p tr√¨nh. "
                            "H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch. "
                            "N·∫øu kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, h√£y ƒë∆∞a ra g·ª£i √Ω ho·∫∑c h∆∞·ªõng d·∫´n t√¨m hi·ªÉu th√™m."
                        )
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000,
                stream=True
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"Error streaming AI: {e}")
            yield "Xin l·ªói, t√¥i g·∫∑p l·ªói khi t·∫°o ph·∫£n h·ªìi. Vui l√≤ng th·ª≠ l·∫°i."

# Kh·ªüi t·∫°o service
chat_ai_service = ChatAIService()

@app.post("/ask")
async def ask(request: ChatRequest):
    """
    API endpoint ch√≠nh ƒë·ªÉ x·ª≠ l√Ω c√¢u h·ªèi chat (non-streaming - gi·ªØ l·∫°i ƒë·ªÉ t∆∞∆°ng th√≠ch)
    """
    try:
        question = request.question.strip()
        
        if not question:
            return {
                "answer": "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω AI h·ªó tr·ª£ h·ªçc l·∫≠p tr√¨nh. B·∫°n mu·ªën h·ªèi g√¨?",
                "data_source": "ai"
            }
        
        # X·ª≠ l√Ω c√¢u h·ªèi th√¥ng qua ChatAI service
        result = chat_ai_service.process_question(question)
        
        return result
        
    except Exception as e:
        logger.error(f"Error in ask endpoint: {e}")
        return {
            "answer": "Xin l·ªói, c√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i sau.",
            "error": str(e)
        }

@app.post("/ask-stream")
async def ask_stream(request: ChatRequest):
    """
    API endpoint streaming ƒë·ªÉ x·ª≠ l√Ω c√¢u h·ªèi chat v·ªõi streaming response
    """
    try:
        question = request.question.strip()
        
        if not question:
            def empty_response():
                yield json.dumps({"type": "error", "content": "C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng"}) + "\n"
            return StreamingResponse(empty_response(), media_type="text/event-stream")
        
        def generate():
            try:
                for chunk in chat_ai_service._stream_ai(
                    f"Ng∆∞·ªùi d√πng h·ªèi: {question}\n\n"
                    f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch th√¢n thi·ªán, chi ti·∫øt v√† h·ªØu √≠ch b·∫±ng ti·∫øng Vi·ªát. "
                    f"B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi h·ªçc l·∫≠p tr√¨nh."
                ):
                    # G·ª≠i t·ª´ng chunk d∆∞·ªõi d·∫°ng JSON
                    data = json.dumps({"type": "chunk", "content": chunk}, ensure_ascii=False)
                    yield f"data: {data}\n\n"
                
                # G·ª≠i signal k·∫øt th√∫c
                yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                logger.error(f"Error in stream generation: {e}")
                error_data = json.dumps({"type": "error", "content": "C√≥ l·ªói x·∫£y ra khi t·∫°o ph·∫£n h·ªìi"}, ensure_ascii=False)
                yield f"data: {error_data}\n\n"
        
        return StreamingResponse(generate(), media_type="text/event-stream")
        
    except Exception as e:
        logger.error(f"Error in ask_stream endpoint: {e}")
        def error_response():
            yield json.dumps({"type": "error", "content": "C√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i sau."}, ensure_ascii=False) + "\n"
        return StreamingResponse(error_response(), media_type="text/event-stream")

@app.get("/health")
async def health_check():
    """
    Health check endpoint
    """
    try:
        # Ki·ªÉm tra k·∫øt n·ªëi AI
        test_response = chat_ai_service._call_ai("Test")
        
        return {
            "status": "healthy",
            "ai_client": "active",
            "message": "AI service is running"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "message": "AI service is not available"
        }
